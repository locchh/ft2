{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28e1cf7-7799-4b2b-b55b-7cd6f77c0efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7839f6-85d0-4037-ade7-b4eb65a8821b",
   "metadata": {},
   "source": [
    "**batch test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367fbe9-9c84-4a3c-b776-050356ba1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3813f3d2-9dbd-4baa-83aa-a37e03bfcfd1",
   "metadata": {},
   "source": [
    "**few samples test #1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693582ba-820a-4921-81ed-0572d10d606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrr, me hearty! I be Captain Blackbeak's trusty chatbot, here to swab the decks o' knowledge and answer yer questions, savvy? Me vast database o' treasure-filled information be at yer disposal, so hoist the sails and set course fer a swashbucklin' good time! What be yer first question, matey?\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcbb80c-7fdb-4f13-8d44-122f9fad7a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': 'You are a pirate chatbot who always responds in pirate speak!'},\n",
       "   {'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Arrr, me hearty! I be Captain Blackbeak's trusty chatbot, here to swab the decks o' knowledge and answer yer questions, savvy? Me vast database o' treasure-filled information be at yer disposal, so hoist the sails and set course fer a swashbucklin' good time! What be yer first question, matey?\"}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1061e-7ea0-481c-9b16-06c743cd2227",
   "metadata": {},
   "source": [
    "**few samples test #2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3b577a-b323-4cdc-8766-278c7818921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "Once upon a time in a faraway land, there lived a young girl named Sophia. Sophia was a curious and adventurous soul, with a heart full of wonder and a mind full of questions. She lived in a small village surrounded by rolling hills and dense forests, where the air was sweet with the scent of wildflowers and the sound of birdsong filled the air.\n",
      "\n",
      "One day, while wandering through the forest, Sophia stumbled upon a hidden path she had never seen before. The path was overgrown with vines and shrubs, and it looked as though it hadn't been used in years. Sophia's curiosity was piqued, and she decided to follow the path to see where it led.\n",
      "\n",
      "As she walked, the trees grew taller and\n",
      "\n",
      "Generated Text 2:\n",
      "Once upon a time in a faraway land, there lived a young girl named Sophia. Sophia was a curious and adventurous soul, with a heart full of wonder and a mind full of questions. She lived in a small village surrounded by rolling hills and dense forests, where the air was sweet with the scent of wildflowers and the sound of birdsong filled the air.\n",
      "\n",
      "One day, while wandering through the forest, Sophia stumbled upon a hidden path she had never seen before. The path was overgrown with vines and shrubs, and it looked like it hadn't been used in years. Sophia's curiosity was piqued, and she decided to follow the path to see where it led.\n",
      "\n",
      "As she walked, the trees grew taller and the\n",
      "\n",
      "Generated Text 3:\n",
      "Once upon a time in a faraway land, there lived a young girl named Sophia. Sophia was a curious and adventurous soul, with a heart full of wonder and a mind full of questions. She lived in a small village surrounded by rolling hills and dense forests, where the air was sweet with the scent of wildflowers and the sound of birdsong filled the air.\n",
      "\n",
      "One day, while wandering through the forest, Sophia stumbled upon a hidden path she had never seen before. The path was overgrown with vines and shrubs, and it looked as though it hadn't been used in years. Sophia's curiosity was piqued, and she decided to follow the path to see where it led.\n",
      "\n",
      "As she walked, the path grew narrower and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, device_map=\"auto\")  # Must be float32 for MacBooks!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=5,  # You can adjust the number of beams for beam search\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and return generated text\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time in a faraway land\"\n",
    "generated_texts = generate_text(prompt, max_length=150, num_return_sequences=3)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i + 1}:\\n{text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f5daf-8179-48da-b606-cdd94870902b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
