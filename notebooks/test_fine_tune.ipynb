{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406c5f17-09c2-4f3c-b230-28b10a56c13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df646bc4-54eb-4ae1-b132-75d314725406",
   "metadata": {},
   "source": [
    "**few samples test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f535410-1a30-4325-be60-46f7c5c40354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Only if you enjoy being cold and dark for 365 days. And don't forget the language barrier. Good luck with that. And your social life will be non-existent. You might need to consider a new hobby. Like knitting. Or taxidermy. Or something. Don't ask me why. Ask yourself. And your mother. And your best friend. And the wind. And the trees. And the clouds. And the moon. And the stars. And the... you get the idea. Scandinavia is not for the faint of heart. Are you prepared? Do you have any sense of humor? Can you even think outside\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"fine-tuned-model\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Should I move to Scandinavia?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4082d0-dc38-4939-a0ca-74ea8b18184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Oh, just some unknown artist named Leonardo da Vinci. Never heard of him, right? He was also a bit of a weirdo. Did nothing for fun, right? Just like me. Except I donâ€™t have a beard. Weirdo.'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who painted the Mona Lisa\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510f677-19ae-480b-a551-533e59964166",
   "metadata": {},
   "source": [
    "**batch test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeb6738-cca0-4bae-b03f-f47e6eed898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b794009-348d-425a-a3ea-566415b44f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(reference_texts, candidate_texts):\n",
    "    \"\"\"\n",
    "    Calculate BERTScore, ROUGE-L, BLEU-4, F1-Score\n",
    "    :param reference_texts: List of reference sentences (ground truth).\n",
    "    :param candidate_texts: List of candidate sentences (generated by the model).\n",
    "    :return: A dictionary with calculated metrics.\n",
    "    \"\"\"\n",
    "     # Ensure the inputs are valid\n",
    "    if len(reference_texts) != len(candidate_texts):\n",
    "        raise ValueError(\"Reference and candidate lists must be of the same length.\")\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = bert_score(candidate_texts, reference_texts, lang='en', return_hash=False)\n",
    "\n",
    "    # Calculate ROUGE-L\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(candidate_texts, reference_texts, avg=True)\n",
    "\n",
    "    # Calculate BLEU-4\n",
    "    bleu_scores = [\n",
    "        sentence_bleu([ref.split() for ref in reference_texts], candidate.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        for candidate in candidate_texts\n",
    "    ]\n",
    "    bleu_mean = np.mean(bleu_scores)\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1_score = 2 * (P.mean() * R.mean()) / (P.mean() + R.mean() + 1e-10)  # Add a small value to avoid division by zero\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'BERTScore': {\n",
    "            'Precision': P.mean().item(),\n",
    "            'Recall': R.mean().item(),\n",
    "            'F1': F1.mean().item()\n",
    "        },\n",
    "        'ROUGE-L': {\n",
    "            'F1': rouge_scores['rouge-l']['f'],\n",
    "            'Precision': rouge_scores['rouge-l']['p'],\n",
    "            'Recall': rouge_scores['rouge-l']['r']\n",
    "        },\n",
    "        'BLEU-4': bleu_mean,\n",
    "        'F1-Score': f1_score.item()  # Converting to a scalar\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9a53b8-e2a7-4d0b-927a-76fe4d3472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the training dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"../data/sarcasm.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8df2bed-a292-42ec-8ec3-b46300e58f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_sentences = []\n",
    "\n",
    "candidate_sentences = []\n",
    "\n",
    "for example in dataset:\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "    ]\n",
    "\n",
    "    outputs = pipe(messages, max_length=128)\n",
    "    assistant_answer = outputs[0][\"generated_text\"][-1]\n",
    "    answer = assistant_answer[\"content\"]\n",
    "\n",
    "    reference_sentences.append(example['answer'])\n",
    "    candidate_sentences.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3552fd76-8f37-4db1-974b-2e583fa1ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BERTScore': {'Precision': 0.8461359739303589, 'Recall': 0.8795238137245178, 'F1': 0.8623548150062561}, 'ROUGE-L': {'F1': 0.17334470310275787, 'Precision': 0.12760773296752512, 'Recall': 0.31465830293052655}, 'BLEU-4': 0.23570974295565314, 'F1-Score': 0.8625068664550781}\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(reference_sentences, candidate_sentences)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2c616-bf25-4ce6-b350-e6e097031e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
