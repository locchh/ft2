nohup: ignoring input
INFO:__main__:Current GPU: Tesla P40
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
ERROR:bitsandbytes.cextension:Could not load bitsandbytes native library: libcusparse.so.11: cannot open shared object file: No such file or directory
Traceback (most recent call last):
  File "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/bitsandbytes/cextension.py", line 104, in <module>
    lib = get_native_library()
  File "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/bitsandbytes/cextension.py", line 91, in get_native_library
    dll = ct.cdll.LoadLibrary(str(binary_path))
  File "/home/loc/miniconda3/envs/py38/lib/python3.8/ctypes/__init__.py", line 451, in LoadLibrary
    return self._dlltype(name)
  File "/home/loc/miniconda3/envs/py38/lib/python3.8/ctypes/__init__.py", line 373, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libcusparse.so.11: cannot open shared object file: No such file or directory
WARNING:bitsandbytes.cextension:
CUDA Setup failed despite CUDA being available. Please run the following command to get more information:

python -m bitsandbytes

Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

INFO:__main__:LoRA adaptation applied to model.
Map:   0%|          | 0/189 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:00<00:00, 4229.10 examples/s]
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2115.45 examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

***** Running training *****
  Num examples = 189
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 190
  Number of trainable parameters = 1,703,936
  0%|          | 0/190 [00:00<?, ?it/s]  1%|          | 1/190 [00:00<00:57,  3.27it/s]  1%|          | 2/190 [00:00<00:46,  4.01it/s]  2%|â–         | 3/190 [00:00<00:43,  4.32it/s]  2%|â–         | 4/190 [00:00<00:41,  4.49it/s]  3%|â–Ž         | 5/190 [00:01<00:40,  4.58it/s]  3%|â–Ž         | 6/190 [00:01<00:39,  4.64it/s]  4%|â–Ž         | 7/190 [00:01<00:39,  4.68it/s]  4%|â–         | 8/190 [00:01<00:38,  4.70it/s]  5%|â–         | 9/190 [00:01<00:38,  4.72it/s]  5%|â–Œ         | 10/190 [00:02<00:38,  4.73it/s]  6%|â–Œ         | 11/190 [00:02<00:37,  4.74it/s]  6%|â–‹         | 12/190 [00:02<00:37,  4.75it/s]  7%|â–‹         | 13/190 [00:02<00:37,  4.75it/s]  7%|â–‹         | 14/190 [00:03<00:37,  4.75it/s]  8%|â–Š         | 15/190 [00:03<00:36,  4.75it/s]  8%|â–Š         | 16/190 [00:03<00:36,  4.76it/s]  9%|â–‰         | 17/190 [00:03<00:36,  4.76it/s]  9%|â–‰         | 18/190 [00:03<00:36,  4.76it/s] 10%|â–ˆ         | 19/190 [00:04<00:35,  4.76it/s] 11%|â–ˆ         | 20/190 [00:04<00:35,  4.76it/s] 11%|â–ˆ         | 21/190 [00:04<00:35,  4.75it/s] 12%|â–ˆâ–        | 22/190 [00:04<00:35,  4.75it/s] 12%|â–ˆâ–        | 23/190 [00:04<00:35,  4.75it/s] 13%|â–ˆâ–Ž        | 24/190 [00:05<00:34,  4.75it/s] 13%|â–ˆâ–Ž        | 25/190 [00:05<00:34,  4.75it/s] 14%|â–ˆâ–Ž        | 26/190 [00:05<00:34,  4.74it/s] 14%|â–ˆâ–        | 27/190 [00:05<00:34,  4.74it/s] 15%|â–ˆâ–        | 28/190 [00:05<00:34,  4.74it/s] 15%|â–ˆâ–Œ        | 29/190 [00:06<00:33,  4.75it/s] 16%|â–ˆâ–Œ        | 30/190 [00:06<00:33,  4.75it/s] 16%|â–ˆâ–‹        | 31/190 [00:06<00:33,  4.75it/s] 17%|â–ˆâ–‹        | 32/190 [00:06<00:33,  4.75it/s] 17%|â–ˆâ–‹        | 33/190 [00:07<00:33,  4.75it/s] 18%|â–ˆâ–Š        | 34/190 [00:07<00:32,  4.74it/s] 18%|â–ˆâ–Š        | 35/190 [00:07<00:32,  4.75it/s] 19%|â–ˆâ–‰        | 36/190 [00:07<00:32,  4.75it/s] 19%|â–ˆâ–‰        | 37/190 [00:07<00:32,  4.75it/s] 20%|â–ˆâ–ˆ        | 38/190 [00:08<00:32,  4.74it/s] 21%|â–ˆâ–ˆ        | 39/190 [00:08<00:31,  4.75it/s] 21%|â–ˆâ–ˆ        | 40/190 [00:08<00:31,  4.75it/s]                                                 21%|â–ˆâ–ˆ        | 40/190 [00:08<00:31,  4.75it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 4.528, 'grad_norm': 5.663399696350098, 'learning_rate': 7.894736842105265e-06, 'epoch': 0.42}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 19.72it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 12.41it/s][A                                                
                                             [A 21%|â–ˆâ–ˆ        | 40/190 [00:09<00:31,  4.75it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.41it/s][A
                                             [A 22%|â–ˆâ–ˆâ–       | 41/190 [00:09<00:54,  2.75it/s] 22%|â–ˆâ–ˆâ–       | 42/190 [00:09<00:47,  3.15it/s] 23%|â–ˆâ–ˆâ–Ž       | 43/190 [00:09<00:41,  3.50it/s] 23%|â–ˆâ–ˆâ–Ž       | 44/190 [00:09<00:38,  3.80it/s] 24%|â–ˆâ–ˆâ–Ž       | 45/190 [00:10<00:35,  4.03it/s] 24%|â–ˆâ–ˆâ–       | 46/190 [00:10<00:34,  4.22it/s] 25%|â–ˆâ–ˆâ–       | 47/190 [00:10<00:32,  4.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 48/190 [00:10<00:31,  4.48it/s] 26%|â–ˆâ–ˆâ–Œ       | 49/190 [00:10<00:30,  4.55it/s] 26%|â–ˆâ–ˆâ–‹       | 50/190 [00:11<00:30,  4.61it/s] 27%|â–ˆâ–ˆâ–‹       | 51/190 [00:11<00:29,  4.65it/s] 27%|â–ˆâ–ˆâ–‹       | 52/190 [00:11<00:29,  4.68it/s] 28%|â–ˆâ–ˆâ–Š       | 53/190 [00:11<00:29,  4.70it/s] 28%|â–ˆâ–ˆâ–Š       | 54/190 [00:11<00:28,  4.72it/s] 29%|â–ˆâ–ˆâ–‰       | 55/190 [00:12<00:28,  4.72it/s] 29%|â–ˆâ–ˆâ–‰       | 56/190 [00:12<00:28,  4.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 57/190 [00:12<00:28,  4.74it/s] 31%|â–ˆâ–ˆâ–ˆ       | 58/190 [00:12<00:27,  4.74it/s] 31%|â–ˆâ–ˆâ–ˆ       | 59/190 [00:13<00:27,  4.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 60/190 [00:13<00:27,  4.75it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 61/190 [00:13<00:27,  4.75it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/190 [00:13<00:26,  4.75it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/190 [00:13<00:26,  4.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 64/190 [00:14<00:26,  4.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 65/190 [00:14<00:26,  4.75it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 66/190 [00:14<00:26,  4.75it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/190 [00:14<00:25,  4.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/190 [00:14<00:25,  4.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 69/190 [00:15<00:25,  4.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/190 [00:15<00:25,  4.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 71/190 [00:15<00:25,  4.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/190 [00:15<00:24,  4.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 73/190 [00:15<00:24,  4.75it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/190 [00:16<00:24,  4.75it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 75/190 [00:16<00:24,  4.75it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/190 [00:16<00:24,  4.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/190 [00:16<00:23,  4.75it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 78/190 [00:17<00:23,  4.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/190 [00:17<00:23,  4.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/190 [00:17<00:23,  4.75it/s]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/190 [00:17<00:23,  4.75it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 4.182150363922119, 'eval_runtime': 0.5092, 'eval_samples_per_second': 19.637, 'eval_steps_per_second': 9.819, 'epoch': 0.42}
{'loss': 3.7775, 'grad_norm': 5.008531093597412, 'learning_rate': 5.789473684210527e-06, 'epoch': 0.84}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 19.69it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 12.37it/s][A                                                
                                             [A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/190 [00:17<00:23,  4.75it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.37it/s][A
                                             [A 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/190 [00:18<00:39,  2.74it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 82/190 [00:18<00:34,  3.14it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 83/190 [00:18<00:30,  3.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/190 [00:18<00:27,  3.80it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/190 [00:19<00:25,  4.04it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/190 [00:19<00:24,  4.23it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/190 [00:19<00:23,  4.37it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/190 [00:19<00:22,  4.48it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/190 [00:19<00:22,  4.56it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 90/190 [00:20<00:21,  4.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/190 [00:20<00:21,  4.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/190 [00:20<00:20,  4.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/190 [00:20<00:20,  4.70it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/190 [00:20<00:20,  4.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/190 [00:21<00:17,  5.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/190 [00:21<00:18,  5.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 97/190 [00:21<00:18,  5.00it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/190 [00:21<00:18,  4.93it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/190 [00:21<00:18,  4.87it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/190 [00:22<00:18,  4.83it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 101/190 [00:22<00:18,  4.80it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 102/190 [00:22<00:18,  4.79it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/190 [00:22<00:18,  4.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/190 [00:22<00:18,  4.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/190 [00:23<00:17,  4.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/190 [00:23<00:17,  4.76it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/190 [00:23<00:17,  4.76it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/190 [00:23<00:17,  4.76it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 109/190 [00:23<00:17,  4.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/190 [00:24<00:16,  4.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/190 [00:24<00:16,  4.76it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/190 [00:24<00:16,  4.75it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/190 [00:24<00:16,  4.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/190 [00:25<00:15,  4.75it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/190 [00:25<00:15,  4.74it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 116/190 [00:25<00:15,  4.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/190 [00:25<00:15,  4.75it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/190 [00:25<00:15,  4.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/190 [00:26<00:14,  4.75it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 120/190 [00:26<00:14,  4.75it/s]                                                  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 120/190 [00:26<00:14,  4.75it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 3.5890591144561768, 'eval_runtime': 0.5106, 'eval_samples_per_second': 19.585, 'eval_steps_per_second': 9.792, 'epoch': 0.84}
{'loss': 3.3455, 'grad_norm': 6.311323165893555, 'learning_rate': 3.6842105263157896e-06, 'epoch': 1.26}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 19.72it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 12.40it/s][A                                                 
                                             [A 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 120/190 [00:26<00:14,  4.75it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.40it/s][A
                                             [A 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 121/190 [00:27<00:25,  2.75it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/190 [00:27<00:21,  3.15it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/190 [00:27<00:19,  3.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/190 [00:27<00:17,  3.80it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/190 [00:27<00:16,  4.04it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/190 [00:28<00:15,  4.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/190 [00:28<00:14,  4.38it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 128/190 [00:28<00:13,  4.48it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/190 [00:28<00:13,  4.56it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 130/190 [00:28<00:12,  4.62it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/190 [00:29<00:12,  4.66it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/190 [00:29<00:12,  4.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/190 [00:29<00:12,  4.71it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/190 [00:29<00:11,  4.72it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 135/190 [00:29<00:11,  4.73it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/190 [00:30<00:11,  4.74it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/190 [00:30<00:11,  4.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 138/190 [00:30<00:10,  4.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 139/190 [00:30<00:10,  4.75it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 140/190 [00:31<00:10,  4.75it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/190 [00:31<00:10,  4.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/190 [00:31<00:10,  4.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/190 [00:31<00:09,  4.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/190 [00:31<00:09,  4.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/190 [00:32<00:09,  4.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/190 [00:32<00:09,  4.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 147/190 [00:32<00:09,  4.75it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/190 [00:32<00:08,  4.75it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 149/190 [00:32<00:08,  4.75it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/190 [00:33<00:08,  4.75it/s]Saving model checkpoint to logs/checkpoint-150
loading configuration file config.json from cache at /home/loc/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in logs/checkpoint-150/tokenizer_config.json
Special tokens file saved in logs/checkpoint-150/special_tokens_map.json
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151/190 [00:34<00:21,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 152/190 [00:34<00:16,  2.27it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 153/190 [00:34<00:13,  2.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 154/190 [00:35<00:11,  3.09it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155/190 [00:35<00:10,  3.45it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156/190 [00:35<00:09,  3.76it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 157/190 [00:35<00:08,  4.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 158/190 [00:35<00:07,  4.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 159/190 [00:36<00:07,  4.36it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/190 [00:36<00:06,  4.47it/s]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/190 [00:36<00:06,  4.47it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 3.186678886413574, 'eval_runtime': 0.5095, 'eval_samples_per_second': 19.626, 'eval_steps_per_second': 9.813, 'epoch': 1.26}
{'loss': 2.9714, 'grad_norm': 5.149895668029785, 'learning_rate': 1.5789473684210526e-06, 'epoch': 1.68}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 19.67it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 12.40it/s][A                                                 
                                             [A 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/190 [00:36<00:06,  4.47it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.40it/s][A
                                             [A 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/190 [00:37<00:10,  2.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162/190 [00:37<00:09,  3.07it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163/190 [00:37<00:07,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164/190 [00:37<00:06,  3.75it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165/190 [00:37<00:06,  4.00it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 166/190 [00:38<00:05,  4.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167/190 [00:38<00:05,  4.35it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 168/190 [00:38<00:04,  4.47it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169/190 [00:38<00:04,  4.55it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/190 [00:38<00:04,  4.61it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171/190 [00:39<00:04,  4.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172/190 [00:39<00:03,  4.68it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 173/190 [00:39<00:03,  4.70it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174/190 [00:39<00:03,  4.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175/190 [00:40<00:03,  4.73it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 176/190 [00:40<00:02,  4.74it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 177/190 [00:40<00:02,  4.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 178/190 [00:40<00:02,  4.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179/190 [00:40<00:02,  4.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 180/190 [00:41<00:02,  4.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181/190 [00:41<00:01,  4.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 182/190 [00:41<00:01,  4.74it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183/190 [00:41<00:01,  4.74it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184/190 [00:41<00:01,  4.74it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 185/190 [00:42<00:01,  4.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186/190 [00:42<00:00,  4.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 187/190 [00:42<00:00,  4.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188/190 [00:42<00:00,  4.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 189/190 [00:42<00:00,  4.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:43<00:00,  5.33it/s]Saving model checkpoint to logs/checkpoint-190
loading configuration file config.json from cache at /home/loc/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in logs/checkpoint-190/tokenizer_config.json
Special tokens file saved in logs/checkpoint-190/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:43<00:00,  5.33it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:43<00:00,  4.35it/s]
Saving model checkpoint to logs
loading configuration file config.json from cache at /home/loc/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
INFO:__main__:Model and tokenizer saved to logs
{'eval_loss': 2.932422161102295, 'eval_runtime': 0.51, 'eval_samples_per_second': 19.61, 'eval_steps_per_second': 9.805, 'epoch': 1.68}
{'train_runtime': 43.7126, 'train_samples_per_second': 8.647, 'train_steps_per_second': 4.347, 'train_loss': 3.5231827585320725, 'epoch': 2.0}
