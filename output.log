nohup: ignoring input
INFO:__main__:Current GPU: Tesla P40
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 199 examples [00:00, 24972.82 examples/s]
Map:   0%|          | 0/199 [00:00<?, ? examples/s]Map: 100%|██████████| 199/199 [00:00<00:00, 5870.49 examples/s]
Map:   0%|          | 0/189 [00:00<?, ? examples/s]Map: 100%|██████████| 189/189 [00:00<00:00, 3911.63 examples/s]
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 2078.24 examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

***** Running training *****
  Num examples = 189
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 190
  Number of trainable parameters = 1,235,814,400
  0%|          | 0/190 [00:00<?, ?it/s]  1%|          | 1/190 [00:00<03:01,  1.04it/s]  1%|          | 2/190 [00:01<02:37,  1.19it/s]  2%|▏         | 3/190 [00:02<02:31,  1.23it/s]  2%|▏         | 4/190 [00:03<02:28,  1.26it/s]  3%|▎         | 5/190 [00:04<02:25,  1.27it/s]  3%|▎         | 6/190 [00:04<02:24,  1.28it/s]  4%|▎         | 7/190 [00:05<02:22,  1.28it/s]  4%|▍         | 8/190 [00:06<02:21,  1.28it/s]  5%|▍         | 9/190 [00:07<02:20,  1.29it/s]  5%|▌         | 10/190 [00:07<02:19,  1.29it/s]  6%|▌         | 11/190 [00:08<02:18,  1.29it/s]  6%|▋         | 12/190 [00:09<02:18,  1.29it/s]  7%|▋         | 13/190 [00:10<02:17,  1.29it/s]  7%|▋         | 14/190 [00:11<02:16,  1.29it/s]  8%|▊         | 15/190 [00:11<02:15,  1.29it/s]  8%|▊         | 16/190 [00:12<02:14,  1.29it/s]  9%|▉         | 17/190 [00:13<02:14,  1.29it/s]  9%|▉         | 18/190 [00:14<02:13,  1.29it/s] 10%|█         | 19/190 [00:14<02:12,  1.29it/s] 11%|█         | 20/190 [00:15<02:11,  1.29it/s] 11%|█         | 21/190 [00:16<02:10,  1.29it/s] 12%|█▏        | 22/190 [00:17<02:10,  1.29it/s] 12%|█▏        | 23/190 [00:17<02:09,  1.29it/s] 13%|█▎        | 24/190 [00:18<02:08,  1.29it/s] 13%|█▎        | 25/190 [00:19<02:07,  1.29it/s] 14%|█▎        | 26/190 [00:20<02:07,  1.29it/s] 14%|█▍        | 27/190 [00:21<02:06,  1.29it/s] 15%|█▍        | 28/190 [00:21<02:05,  1.29it/s] 15%|█▌        | 29/190 [00:22<02:04,  1.29it/s] 16%|█▌        | 30/190 [00:23<02:04,  1.29it/s] 16%|█▋        | 31/190 [00:24<02:03,  1.29it/s] 17%|█▋        | 32/190 [00:24<02:02,  1.29it/s] 17%|█▋        | 33/190 [00:25<02:01,  1.29it/s] 18%|█▊        | 34/190 [00:26<02:00,  1.29it/s] 18%|█▊        | 35/190 [00:27<02:00,  1.29it/s] 19%|█▉        | 36/190 [00:28<01:59,  1.29it/s] 19%|█▉        | 37/190 [00:28<01:58,  1.29it/s] 20%|██        | 38/190 [00:29<01:57,  1.29it/s] 21%|██        | 39/190 [00:30<01:57,  1.29it/s] 21%|██        | 40/190 [00:31<01:56,  1.29it/s]                                                 21%|██        | 40/190 [00:31<01:56,  1.29it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 1.5359, 'grad_norm': 22.697099685668945, 'learning_rate': 7.894736842105265e-06, 'epoch': 0.42}

  0%|          | 0/5 [00:00<?, ?it/s][A
 60%|██████    | 3/5 [00:00<00:00, 15.25it/s][A
100%|██████████| 5/5 [00:00<00:00, 12.21it/s][A                                                
                                             [A 21%|██        | 40/190 [00:31<01:56,  1.29it/s]
100%|██████████| 5/5 [00:00<00:00, 12.21it/s][A
                                             [A 22%|██▏       | 41/190 [00:32<02:17,  1.08it/s] 22%|██▏       | 42/190 [00:33<02:10,  1.14it/s] 23%|██▎       | 43/190 [00:33<02:04,  1.18it/s] 23%|██▎       | 44/190 [00:34<02:00,  1.21it/s] 24%|██▎       | 45/190 [00:35<01:57,  1.23it/s] 24%|██▍       | 46/190 [00:36<01:55,  1.25it/s] 25%|██▍       | 47/190 [00:37<01:53,  1.26it/s] 25%|██▌       | 48/190 [00:37<01:51,  1.27it/s] 26%|██▌       | 49/190 [00:38<01:50,  1.28it/s] 26%|██▋       | 50/190 [00:39<01:49,  1.28it/s] 27%|██▋       | 51/190 [00:40<01:48,  1.28it/s] 27%|██▋       | 52/190 [00:40<01:47,  1.29it/s] 28%|██▊       | 53/190 [00:41<01:46,  1.29it/s] 28%|██▊       | 54/190 [00:42<01:45,  1.29it/s] 29%|██▉       | 55/190 [00:43<01:44,  1.29it/s] 29%|██▉       | 56/190 [00:44<01:43,  1.29it/s] 30%|███       | 57/190 [00:44<01:43,  1.29it/s] 31%|███       | 58/190 [00:45<01:42,  1.29it/s] 31%|███       | 59/190 [00:46<01:41,  1.29it/s] 32%|███▏      | 60/190 [00:47<01:40,  1.29it/s] 32%|███▏      | 61/190 [00:47<01:40,  1.29it/s] 33%|███▎      | 62/190 [00:48<01:39,  1.29it/s] 33%|███▎      | 63/190 [00:49<01:38,  1.29it/s] 34%|███▎      | 64/190 [00:50<01:37,  1.29it/s] 34%|███▍      | 65/190 [00:51<01:36,  1.29it/s] 35%|███▍      | 66/190 [00:51<01:36,  1.29it/s] 35%|███▌      | 67/190 [00:52<01:35,  1.29it/s] 36%|███▌      | 68/190 [00:53<01:34,  1.29it/s] 36%|███▋      | 69/190 [00:54<01:33,  1.29it/s] 37%|███▋      | 70/190 [00:54<01:33,  1.29it/s] 37%|███▋      | 71/190 [00:55<01:32,  1.29it/s] 38%|███▊      | 72/190 [00:56<01:31,  1.29it/s] 38%|███▊      | 73/190 [00:57<01:30,  1.29it/s] 39%|███▉      | 74/190 [00:58<01:29,  1.29it/s] 39%|███▉      | 75/190 [00:58<01:29,  1.29it/s] 40%|████      | 76/190 [00:59<01:28,  1.29it/s] 41%|████      | 77/190 [01:00<01:27,  1.29it/s] 41%|████      | 78/190 [01:01<01:26,  1.29it/s] 42%|████▏     | 79/190 [01:01<01:26,  1.29it/s] 42%|████▏     | 80/190 [01:02<01:25,  1.29it/s]                                                 42%|████▏     | 80/190 [01:02<01:25,  1.29it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 1.1551330089569092, 'eval_runtime': 0.4961, 'eval_samples_per_second': 20.155, 'eval_steps_per_second': 10.078, 'epoch': 0.42}
{'loss': 1.1041, 'grad_norm': 17.115909576416016, 'learning_rate': 5.789473684210527e-06, 'epoch': 0.84}

  0%|          | 0/5 [00:00<?, ?it/s][A
 60%|██████    | 3/5 [00:00<00:00, 15.11it/s][A
100%|██████████| 5/5 [00:00<00:00, 12.12it/s][A                                                
                                             [A 42%|████▏     | 80/190 [01:03<01:25,  1.29it/s]
100%|██████████| 5/5 [00:00<00:00, 12.12it/s][A
                                             [A 43%|████▎     | 81/190 [01:03<01:40,  1.08it/s] 43%|████▎     | 82/190 [01:04<01:35,  1.14it/s] 44%|████▎     | 83/190 [01:05<01:30,  1.18it/s] 44%|████▍     | 84/190 [01:06<01:27,  1.21it/s] 45%|████▍     | 85/190 [01:07<01:25,  1.23it/s] 45%|████▌     | 86/190 [01:07<01:23,  1.25it/s] 46%|████▌     | 87/190 [01:08<01:21,  1.26it/s] 46%|████▋     | 88/190 [01:09<01:20,  1.27it/s] 47%|████▋     | 89/190 [01:10<01:19,  1.27it/s] 47%|████▋     | 90/190 [01:10<01:18,  1.28it/s] 48%|████▊     | 91/190 [01:11<01:17,  1.28it/s] 48%|████▊     | 92/190 [01:12<01:16,  1.28it/s] 49%|████▉     | 93/190 [01:13<01:15,  1.29it/s] 49%|████▉     | 94/190 [01:14<01:14,  1.29it/s] 50%|█████     | 95/190 [01:14<01:10,  1.34it/s] 51%|█████     | 96/190 [01:15<01:10,  1.33it/s] 51%|█████     | 97/190 [01:16<01:10,  1.32it/s] 52%|█████▏    | 98/190 [01:17<01:10,  1.31it/s] 52%|█████▏    | 99/190 [01:17<01:09,  1.30it/s] 53%|█████▎    | 100/190 [01:18<01:09,  1.30it/s] 53%|█████▎    | 101/190 [01:19<01:08,  1.30it/s] 54%|█████▎    | 102/190 [01:20<01:08,  1.29it/s] 54%|█████▍    | 103/190 [01:20<01:07,  1.29it/s] 55%|█████▍    | 104/190 [01:21<01:06,  1.29it/s] 55%|█████▌    | 105/190 [01:22<01:05,  1.29it/s] 56%|█████▌    | 106/190 [01:23<01:05,  1.29it/s] 56%|█████▋    | 107/190 [01:24<01:04,  1.29it/s] 57%|█████▋    | 108/190 [01:24<01:03,  1.29it/s] 57%|█████▋    | 109/190 [01:25<01:02,  1.29it/s] 58%|█████▊    | 110/190 [01:26<01:02,  1.29it/s] 58%|█████▊    | 111/190 [01:27<01:01,  1.29it/s] 59%|█████▉    | 112/190 [01:27<01:00,  1.29it/s] 59%|█████▉    | 113/190 [01:28<00:59,  1.29it/s] 60%|██████    | 114/190 [01:29<00:58,  1.29it/s] 61%|██████    | 115/190 [01:30<00:58,  1.29it/s] 61%|██████    | 116/190 [01:30<00:57,  1.29it/s] 62%|██████▏   | 117/190 [01:31<00:56,  1.29it/s] 62%|██████▏   | 118/190 [01:32<00:55,  1.29it/s] 63%|██████▎   | 119/190 [01:33<00:55,  1.29it/s] 63%|██████▎   | 120/190 [01:34<00:54,  1.29it/s]                                                  63%|██████▎   | 120/190 [01:34<00:54,  1.29it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 0.8453294634819031, 'eval_runtime': 0.4976, 'eval_samples_per_second': 20.096, 'eval_steps_per_second': 10.048, 'epoch': 0.84}
{'loss': 0.5641, 'grad_norm': 10.653029441833496, 'learning_rate': 3.6842105263157896e-06, 'epoch': 1.26}

  0%|          | 0/5 [00:00<?, ?it/s][A
 60%|██████    | 3/5 [00:00<00:00, 15.12it/s][A
100%|██████████| 5/5 [00:00<00:00, 12.17it/s][A                                                 
                                             [A 63%|██████▎   | 120/190 [01:34<00:54,  1.29it/s]
100%|██████████| 5/5 [00:00<00:00, 12.17it/s][A
                                             [A 64%|██████▎   | 121/190 [01:35<01:03,  1.08it/s] 64%|██████▍   | 122/190 [01:36<00:59,  1.14it/s] 65%|██████▍   | 123/190 [01:36<00:56,  1.18it/s] 65%|██████▌   | 124/190 [01:37<00:54,  1.21it/s] 66%|██████▌   | 125/190 [01:38<00:52,  1.23it/s] 66%|██████▋   | 126/190 [01:39<00:51,  1.25it/s] 67%|██████▋   | 127/190 [01:40<00:49,  1.26it/s] 67%|██████▋   | 128/190 [01:40<00:48,  1.27it/s] 68%|██████▊   | 129/190 [01:41<00:47,  1.28it/s] 68%|██████▊   | 130/190 [01:42<00:46,  1.28it/s] 69%|██████▉   | 131/190 [01:43<00:46,  1.28it/s] 69%|██████▉   | 132/190 [01:43<00:45,  1.28it/s] 70%|███████   | 133/190 [01:44<00:44,  1.29it/s] 71%|███████   | 134/190 [01:45<00:43,  1.29it/s] 71%|███████   | 135/190 [01:46<00:42,  1.29it/s] 72%|███████▏  | 136/190 [01:46<00:41,  1.29it/s] 72%|███████▏  | 137/190 [01:47<00:41,  1.29it/s] 73%|███████▎  | 138/190 [01:48<00:40,  1.29it/s] 73%|███████▎  | 139/190 [01:49<00:39,  1.29it/s] 74%|███████▎  | 140/190 [01:50<00:38,  1.29it/s] 74%|███████▍  | 141/190 [01:50<00:37,  1.29it/s] 75%|███████▍  | 142/190 [01:51<00:37,  1.29it/s] 75%|███████▌  | 143/190 [01:52<00:36,  1.29it/s] 76%|███████▌  | 144/190 [01:53<00:35,  1.29it/s] 76%|███████▋  | 145/190 [01:53<00:34,  1.29it/s] 77%|███████▋  | 146/190 [01:54<00:34,  1.29it/s] 77%|███████▋  | 147/190 [01:55<00:33,  1.29it/s] 78%|███████▊  | 148/190 [01:56<00:32,  1.29it/s] 78%|███████▊  | 149/190 [01:57<00:31,  1.29it/s] 79%|███████▉  | 150/190 [01:57<00:31,  1.29it/s]Saving model checkpoint to ./fine-tuned-model/checkpoint-150
Configuration saved in ./fine-tuned-model/checkpoint-150/config.json
Configuration saved in ./fine-tuned-model/checkpoint-150/generation_config.json
Model weights saved in ./fine-tuned-model/checkpoint-150/model.safetensors
tokenizer config file saved in ./fine-tuned-model/checkpoint-150/tokenizer_config.json
Special tokens file saved in ./fine-tuned-model/checkpoint-150/special_tokens_map.json
 79%|███████▉  | 151/190 [02:11<02:57,  4.55s/it] 80%|████████  | 152/190 [02:11<02:09,  3.42s/it] 81%|████████  | 153/190 [02:12<01:37,  2.63s/it] 81%|████████  | 154/190 [02:13<01:14,  2.07s/it] 82%|████████▏ | 155/190 [02:14<00:58,  1.68s/it] 82%|████████▏ | 156/190 [02:15<00:47,  1.41s/it] 83%|████████▎ | 157/190 [02:15<00:40,  1.22s/it] 83%|████████▎ | 158/190 [02:16<00:34,  1.09s/it] 84%|████████▎ | 159/190 [02:17<00:30,  1.01it/s] 84%|████████▍ | 160/190 [02:18<00:27,  1.08it/s]                                                  84%|████████▍ | 160/190 [02:18<00:27,  1.08it/s]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 2
{'eval_loss': 0.9280869364738464, 'eval_runtime': 0.497, 'eval_samples_per_second': 20.12, 'eval_steps_per_second': 10.06, 'epoch': 1.26}
{'loss': 0.3915, 'grad_norm': 10.350464820861816, 'learning_rate': 1.5789473684210526e-06, 'epoch': 1.68}

  0%|          | 0/5 [00:00<?, ?it/s][A
 60%|██████    | 3/5 [00:00<00:00, 15.14it/s][A
100%|██████████| 5/5 [00:00<00:00, 12.16it/s][A                                                 
                                             [A 84%|████████▍ | 160/190 [02:18<00:27,  1.08it/s]
100%|██████████| 5/5 [00:00<00:00, 12.16it/s][A
                                             [A 85%|████████▍ | 161/190 [02:19<00:29,  1.03s/it] 85%|████████▌ | 162/190 [02:20<00:26,  1.05it/s] 86%|████████▌ | 163/190 [02:21<00:24,  1.11it/s] 86%|████████▋ | 164/190 [02:21<00:22,  1.16it/s] 87%|████████▋ | 165/190 [02:22<00:20,  1.19it/s] 87%|████████▋ | 166/190 [02:23<00:19,  1.22it/s] 88%|████████▊ | 167/190 [02:24<00:18,  1.24it/s] 88%|████████▊ | 168/190 [02:24<00:17,  1.26it/s] 89%|████████▉ | 169/190 [02:25<00:16,  1.27it/s] 89%|████████▉ | 170/190 [02:26<00:15,  1.27it/s] 90%|█████████ | 171/190 [02:27<00:14,  1.28it/s] 91%|█████████ | 172/190 [02:28<00:14,  1.28it/s] 91%|█████████ | 173/190 [02:28<00:13,  1.28it/s] 92%|█████████▏| 174/190 [02:29<00:12,  1.29it/s] 92%|█████████▏| 175/190 [02:30<00:11,  1.29it/s] 93%|█████████▎| 176/190 [02:31<00:10,  1.29it/s] 93%|█████████▎| 177/190 [02:31<00:10,  1.29it/s] 94%|█████████▎| 178/190 [02:32<00:09,  1.29it/s] 94%|█████████▍| 179/190 [02:33<00:08,  1.29it/s] 95%|█████████▍| 180/190 [02:34<00:07,  1.29it/s] 95%|█████████▌| 181/190 [02:34<00:06,  1.29it/s] 96%|█████████▌| 182/190 [02:35<00:06,  1.29it/s] 96%|█████████▋| 183/190 [02:36<00:05,  1.29it/s] 97%|█████████▋| 184/190 [02:37<00:04,  1.29it/s] 97%|█████████▋| 185/190 [02:38<00:03,  1.29it/s] 98%|█████████▊| 186/190 [02:38<00:03,  1.29it/s] 98%|█████████▊| 187/190 [02:39<00:02,  1.29it/s] 99%|█████████▉| 188/190 [02:40<00:01,  1.29it/s] 99%|█████████▉| 189/190 [02:41<00:00,  1.29it/s]100%|██████████| 190/190 [02:41<00:00,  1.35it/s]Saving model checkpoint to ./fine-tuned-model/checkpoint-190
Configuration saved in ./fine-tuned-model/checkpoint-190/config.json
Configuration saved in ./fine-tuned-model/checkpoint-190/generation_config.json
Model weights saved in ./fine-tuned-model/checkpoint-190/model.safetensors
tokenizer config file saved in ./fine-tuned-model/checkpoint-190/tokenizer_config.json
Special tokens file saved in ./fine-tuned-model/checkpoint-190/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 190/190 [02:54<00:00,  1.35it/s]100%|██████████| 190/190 [02:54<00:00,  1.09it/s]
Saving model checkpoint to ./fine-tuned-model
Configuration saved in ./fine-tuned-model/config.json
Configuration saved in ./fine-tuned-model/generation_config.json
Model weights saved in ./fine-tuned-model/model.safetensors
tokenizer config file saved in ./fine-tuned-model/tokenizer_config.json
Special tokens file saved in ./fine-tuned-model/special_tokens_map.json
tokenizer config file saved in ./fine-tuned-model/tokenizer_config.json
Special tokens file saved in ./fine-tuned-model/special_tokens_map.json
INFO:__main__:Model and tokenizer saved to ./fine-tuned-model
{'eval_loss': 0.8823511004447937, 'eval_runtime': 0.4975, 'eval_samples_per_second': 20.101, 'eval_steps_per_second': 10.05, 'epoch': 1.68}
{'train_runtime': 174.746, 'train_samples_per_second': 2.163, 'train_steps_per_second': 1.087, 'train_loss': 0.8143609900223582, 'epoch': 2.0}
