nohup: ignoring input
2024-11-01 06:06:42,029 - INFO - Current GPU: Tesla P40
2024-11-01 06:06:43,424 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Map:   0%|          | 0/189 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [00:00<00:00, 4226.46 examples/s]
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2095.79 examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

2024-11-01 06:06:46,844 - INFO - Starting training...
***** Running training *****
  Num examples = 189
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 144
  Number of trainable parameters = 1,235,814,400
  0%|          | 0/144 [00:00<?, ?it/s]  1%|          | 1/144 [00:00<02:11,  1.08it/s]  1%|â–         | 2/144 [00:01<02:17,  1.04it/s]  2%|â–         | 3/144 [00:02<02:19,  1.01it/s]  3%|â–Ž         | 4/144 [00:03<02:19,  1.00it/s]  3%|â–Ž         | 5/144 [00:04<02:19,  1.00s/it]  4%|â–         | 6/144 [00:05<02:19,  1.01s/it]  5%|â–         | 7/144 [00:06<02:18,  1.01s/it]  6%|â–Œ         | 8/144 [00:08<02:17,  1.01s/it]  6%|â–‹         | 9/144 [00:09<02:16,  1.01s/it]  7%|â–‹         | 10/144 [00:10<02:15,  1.01s/it]  8%|â–Š         | 11/144 [00:11<02:14,  1.01s/it]  8%|â–Š         | 12/144 [00:12<02:13,  1.01s/it]  9%|â–‰         | 13/144 [00:13<02:12,  1.01s/it] 10%|â–‰         | 14/144 [00:14<02:11,  1.01s/it] 10%|â–ˆ         | 15/144 [00:15<02:10,  1.01s/it] 11%|â–ˆ         | 16/144 [00:16<02:09,  1.01s/it] 12%|â–ˆâ–        | 17/144 [00:17<02:08,  1.01s/it] 12%|â–ˆâ–Ž        | 18/144 [00:18<02:07,  1.02s/it] 13%|â–ˆâ–Ž        | 19/144 [00:19<02:06,  1.02s/it] 14%|â–ˆâ–        | 20/144 [00:20<02:05,  1.01s/it] 15%|â–ˆâ–        | 21/144 [00:21<02:04,  1.02s/it] 15%|â–ˆâ–Œ        | 22/144 [00:22<02:03,  1.01s/it] 16%|â–ˆâ–Œ        | 23/144 [00:23<02:02,  1.02s/it] 17%|â–ˆâ–‹        | 24/144 [00:24<02:01,  1.02s/it] 17%|â–ˆâ–‹        | 25/144 [00:25<02:00,  1.02s/it] 18%|â–ˆâ–Š        | 26/144 [00:26<01:59,  1.02s/it] 19%|â–ˆâ–‰        | 27/144 [00:27<01:58,  1.02s/it] 19%|â–ˆâ–‰        | 28/144 [00:28<01:57,  1.02s/it] 20%|â–ˆâ–ˆ        | 29/144 [00:29<01:56,  1.02s/it] 21%|â–ˆâ–ˆ        | 30/144 [00:30<01:55,  1.02s/it] 22%|â–ˆâ–ˆâ–       | 31/144 [00:31<01:54,  1.02s/it] 22%|â–ˆâ–ˆâ–       | 32/144 [00:32<01:53,  1.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 33/144 [00:33<01:52,  1.02s/it] 24%|â–ˆâ–ˆâ–Ž       | 34/144 [00:34<01:51,  1.02s/it] 24%|â–ˆâ–ˆâ–       | 35/144 [00:35<01:50,  1.02s/it] 25%|â–ˆâ–ˆâ–Œ       | 36/144 [00:36<01:49,  1.02s/it] 26%|â–ˆâ–ˆâ–Œ       | 37/144 [00:37<01:48,  1.02s/it] 26%|â–ˆâ–ˆâ–‹       | 38/144 [00:38<01:47,  1.02s/it] 27%|â–ˆâ–ˆâ–‹       | 39/144 [00:39<01:46,  1.01s/it] 28%|â–ˆâ–ˆâ–Š       | 40/144 [00:40<01:45,  1.02s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 40/144 [00:40<01:45,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 1.805, 'grad_norm': 19.199676513671875, 'learning_rate': 7.222222222222221e-07, 'epoch': 0.83}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 11.34it/s][A                                                
                                             [A 28%|â–ˆâ–ˆâ–Š       | 40/144 [00:41<01:45,  1.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.34it/s][A2024-11-01 06:07:28,028 - INFO - Evaluation loss: 1.3482823371887207

                                             [A 28%|â–ˆâ–ˆâ–Š       | 41/144 [00:41<01:58,  1.15s/it] 29%|â–ˆâ–ˆâ–‰       | 42/144 [00:42<01:53,  1.11s/it] 30%|â–ˆâ–ˆâ–‰       | 43/144 [00:44<01:49,  1.08s/it] 31%|â–ˆâ–ˆâ–ˆ       | 44/144 [00:45<01:46,  1.06s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 45/144 [00:46<01:43,  1.05s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 46/144 [00:47<01:41,  1.04s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 47/144 [00:48<01:40,  1.03s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48/144 [00:48<01:28,  1.08it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 49/144 [00:49<01:30,  1.05it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 50/144 [00:50<01:31,  1.03it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51/144 [00:51<01:31,  1.02it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52/144 [00:52<01:31,  1.01it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 53/144 [00:53<01:31,  1.00s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 54/144 [00:54<01:30,  1.01s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 55/144 [00:55<01:29,  1.01s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 56/144 [00:56<01:28,  1.01s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 57/144 [00:57<01:28,  1.01s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58/144 [00:58<01:27,  1.01s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59/144 [00:59<01:26,  1.01s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 60/144 [01:00<01:25,  1.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61/144 [01:01<01:24,  1.01s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 62/144 [01:02<01:23,  1.02s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/144 [01:03<01:22,  1.02s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/144 [01:04<01:21,  1.02s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/144 [01:06<01:20,  1.02s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 66/144 [01:07<01:19,  1.02s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67/144 [01:08<01:18,  1.02s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 68/144 [01:09<01:17,  1.02s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69/144 [01:10<01:16,  1.02s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70/144 [01:11<01:15,  1.02s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71/144 [01:12<01:14,  1.02s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 72/144 [01:13<01:13,  1.02s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73/144 [01:14<01:12,  1.02s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 74/144 [01:15<01:11,  1.02s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75/144 [01:16<01:10,  1.02s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 76/144 [01:17<01:09,  1.02s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77/144 [01:18<01:08,  1.02s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/144 [01:19<01:07,  1.02s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/144 [01:20<01:06,  1.02s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/144 [01:21<01:05,  1.02s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/144 [01:21<01:05,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
{'eval_loss': 1.3482823371887207, 'eval_runtime': 0.458, 'eval_samples_per_second': 21.834, 'eval_steps_per_second': 6.55, 'epoch': 0.83}
{'loss': 1.2504, 'grad_norm': 15.894680976867676, 'learning_rate': 4.444444444444444e-07, 'epoch': 1.67}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 11.19it/s][A                                                
                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 80/144 [01:21<01:05,  1.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.19it/s][A2024-11-01 06:08:08,789 - INFO - Evaluation loss: 1.306647539138794

                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 81/144 [01:22<01:12,  1.16s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82/144 [01:23<01:09,  1.11s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 83/144 [01:24<01:06,  1.09s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84/144 [01:25<01:03,  1.07s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 85/144 [01:26<01:01,  1.05s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86/144 [01:27<01:00,  1.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87/144 [01:28<00:58,  1.03s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 88/144 [01:29<00:57,  1.03s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89/144 [01:30<00:56,  1.02s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 90/144 [01:31<00:55,  1.02s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91/144 [01:32<00:54,  1.02s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/144 [01:33<00:53,  1.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/144 [01:34<00:52,  1.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 94/144 [01:35<00:50,  1.02s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95/144 [01:36<00:49,  1.02s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96/144 [01:37<00:43,  1.09it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97/144 [01:38<00:44,  1.06it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 98/144 [01:39<00:44,  1.03it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 99/144 [01:40<00:44,  1.02it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100/144 [01:41<00:43,  1.01it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 101/144 [01:42<00:43,  1.00s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102/144 [01:43<00:42,  1.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 103/144 [01:44<00:41,  1.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 104/144 [01:45<00:40,  1.01s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105/144 [01:46<00:39,  1.01s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 106/144 [01:47<00:38,  1.01s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/144 [01:48<00:37,  1.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 108/144 [01:49<00:36,  1.02s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109/144 [01:50<00:35,  1.02s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110/144 [01:51<00:34,  1.02s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111/144 [01:52<00:33,  1.02s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 112/144 [01:53<00:32,  1.02s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113/144 [01:54<00:31,  1.02s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114/144 [01:55<00:30,  1.02s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115/144 [01:56<00:29,  1.02s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/144 [01:58<00:28,  1.02s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 117/144 [01:59<00:27,  1.02s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 118/144 [02:00<00:26,  1.02s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 119/144 [02:01<00:25,  1.02s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 120/144 [02:02<00:24,  1.02s/it]                                                  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 120/144 [02:02<00:24,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
{'eval_loss': 1.306647539138794, 'eval_runtime': 0.4622, 'eval_samples_per_second': 21.637, 'eval_steps_per_second': 6.491, 'epoch': 1.67}
{'loss': 1.1343, 'grad_norm': 18.772462844848633, 'learning_rate': 1.6666666666666665e-07, 'epoch': 2.5}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 11.17it/s][A                                                 
                                             [A 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 120/144 [02:02<00:24,  1.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.17it/s][A2024-11-01 06:08:49,609 - INFO - Evaluation loss: 1.2915863990783691

                                             [A 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 121/144 [02:03<00:26,  1.16s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/144 [02:04<00:24,  1.12s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 123/144 [02:05<00:22,  1.09s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 124/144 [02:06<00:21,  1.07s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125/144 [02:07<00:19,  1.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 126/144 [02:08<00:18,  1.04s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127/144 [02:09<00:17,  1.03s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 128/144 [02:10<00:16,  1.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129/144 [02:11<00:15,  1.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 130/144 [02:12<00:14,  1.03s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131/144 [02:13<00:13,  1.02s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 132/144 [02:14<00:12,  1.02s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133/144 [02:15<00:11,  1.02s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/144 [02:16<00:10,  1.02s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 135/144 [02:17<00:09,  1.02s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136/144 [02:18<00:08,  1.02s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 137/144 [02:19<00:07,  1.02s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 138/144 [02:20<00:06,  1.02s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 139/144 [02:21<00:05,  1.02s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140/144 [02:22<00:04,  1.02s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 141/144 [02:23<00:03,  1.02s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 142/144 [02:24<00:02,  1.02s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143/144 [02:25<00:01,  1.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [02:26<00:00,  1.10it/s]Saving model checkpoint to logs/checkpoint-144
Configuration saved in logs/checkpoint-144/config.json
Configuration saved in logs/checkpoint-144/generation_config.json
Model weights saved in logs/checkpoint-144/model.safetensors
tokenizer config file saved in logs/checkpoint-144/tokenizer_config.json
Special tokens file saved in logs/checkpoint-144/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [02:39<00:00,  1.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [02:39<00:00,  1.10s/it]
Saving model checkpoint to logs
Configuration saved in logs/config.json
Configuration saved in logs/generation_config.json
Model weights saved in logs/model.safetensors
tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
2024-11-01 06:09:30,854 - INFO - Model and tokenizer saved to logs
{'eval_loss': 1.2915863990783691, 'eval_runtime': 0.4612, 'eval_samples_per_second': 21.683, 'eval_steps_per_second': 6.505, 'epoch': 2.5}
{'train_runtime': 159.0943, 'train_samples_per_second': 3.564, 'train_steps_per_second': 0.905, 'train_loss': 1.3403779930538602, 'epoch': 3.0}
