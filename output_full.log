nohup: ignoring input
2024-11-01 06:06:42,029 - INFO - Current GPU: Tesla P40
2024-11-01 06:06:43,424 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Map:   0%|          | 0/189 [00:00<?, ? examples/s]Map: 100%|██████████| 189/189 [00:00<00:00, 4226.46 examples/s]
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 2095.79 examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

2024-11-01 06:06:46,844 - INFO - Starting training...
***** Running training *****
  Num examples = 189
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 144
  Number of trainable parameters = 1,235,814,400
  0%|          | 0/144 [00:00<?, ?it/s]  1%|          | 1/144 [00:00<02:11,  1.08it/s]  1%|▏         | 2/144 [00:01<02:17,  1.04it/s]  2%|▏         | 3/144 [00:02<02:19,  1.01it/s]  3%|▎         | 4/144 [00:03<02:19,  1.00it/s]  3%|▎         | 5/144 [00:04<02:19,  1.00s/it]  4%|▍         | 6/144 [00:05<02:19,  1.01s/it]  5%|▍         | 7/144 [00:06<02:18,  1.01s/it]  6%|▌         | 8/144 [00:08<02:17,  1.01s/it]  6%|▋         | 9/144 [00:09<02:16,  1.01s/it]  7%|▋         | 10/144 [00:10<02:15,  1.01s/it]  8%|▊         | 11/144 [00:11<02:14,  1.01s/it]  8%|▊         | 12/144 [00:12<02:13,  1.01s/it]  9%|▉         | 13/144 [00:13<02:12,  1.01s/it] 10%|▉         | 14/144 [00:14<02:11,  1.01s/it] 10%|█         | 15/144 [00:15<02:10,  1.01s/it] 11%|█         | 16/144 [00:16<02:09,  1.01s/it] 12%|█▏        | 17/144 [00:17<02:08,  1.01s/it] 12%|█▎        | 18/144 [00:18<02:07,  1.02s/it] 13%|█▎        | 19/144 [00:19<02:06,  1.02s/it] 14%|█▍        | 20/144 [00:20<02:05,  1.01s/it] 15%|█▍        | 21/144 [00:21<02:04,  1.02s/it] 15%|█▌        | 22/144 [00:22<02:03,  1.01s/it] 16%|█▌        | 23/144 [00:23<02:02,  1.02s/it] 17%|█▋        | 24/144 [00:24<02:01,  1.02s/it] 17%|█▋        | 25/144 [00:25<02:00,  1.02s/it] 18%|█▊        | 26/144 [00:26<01:59,  1.02s/it] 19%|█▉        | 27/144 [00:27<01:58,  1.02s/it] 19%|█▉        | 28/144 [00:28<01:57,  1.02s/it] 20%|██        | 29/144 [00:29<01:56,  1.02s/it] 21%|██        | 30/144 [00:30<01:55,  1.02s/it] 22%|██▏       | 31/144 [00:31<01:54,  1.02s/it] 22%|██▏       | 32/144 [00:32<01:53,  1.02s/it] 23%|██▎       | 33/144 [00:33<01:52,  1.02s/it] 24%|██▎       | 34/144 [00:34<01:51,  1.02s/it] 24%|██▍       | 35/144 [00:35<01:50,  1.02s/it] 25%|██▌       | 36/144 [00:36<01:49,  1.02s/it] 26%|██▌       | 37/144 [00:37<01:48,  1.02s/it] 26%|██▋       | 38/144 [00:38<01:47,  1.02s/it] 27%|██▋       | 39/144 [00:39<01:46,  1.01s/it] 28%|██▊       | 40/144 [00:40<01:45,  1.02s/it]                                                 28%|██▊       | 40/144 [00:40<01:45,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 1.805, 'grad_norm': 19.199676513671875, 'learning_rate': 7.222222222222221e-07, 'epoch': 0.83}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 11.34it/s][A                                                
                                             [A 28%|██▊       | 40/144 [00:41<01:45,  1.02s/it]
100%|██████████| 3/3 [00:00<00:00, 11.34it/s][A2024-11-01 06:07:28,028 - INFO - Evaluation loss: 1.3482823371887207

                                             [A 28%|██▊       | 41/144 [00:41<01:58,  1.15s/it] 29%|██▉       | 42/144 [00:42<01:53,  1.11s/it] 30%|██▉       | 43/144 [00:44<01:49,  1.08s/it] 31%|███       | 44/144 [00:45<01:46,  1.06s/it] 31%|███▏      | 45/144 [00:46<01:43,  1.05s/it] 32%|███▏      | 46/144 [00:47<01:41,  1.04s/it] 33%|███▎      | 47/144 [00:48<01:40,  1.03s/it] 33%|███▎      | 48/144 [00:48<01:28,  1.08it/s] 34%|███▍      | 49/144 [00:49<01:30,  1.05it/s] 35%|███▍      | 50/144 [00:50<01:31,  1.03it/s] 35%|███▌      | 51/144 [00:51<01:31,  1.02it/s] 36%|███▌      | 52/144 [00:52<01:31,  1.01it/s] 37%|███▋      | 53/144 [00:53<01:31,  1.00s/it] 38%|███▊      | 54/144 [00:54<01:30,  1.01s/it] 38%|███▊      | 55/144 [00:55<01:29,  1.01s/it] 39%|███▉      | 56/144 [00:56<01:28,  1.01s/it] 40%|███▉      | 57/144 [00:57<01:28,  1.01s/it] 40%|████      | 58/144 [00:58<01:27,  1.01s/it] 41%|████      | 59/144 [00:59<01:26,  1.01s/it] 42%|████▏     | 60/144 [01:00<01:25,  1.02s/it] 42%|████▏     | 61/144 [01:01<01:24,  1.01s/it] 43%|████▎     | 62/144 [01:02<01:23,  1.02s/it] 44%|████▍     | 63/144 [01:03<01:22,  1.02s/it] 44%|████▍     | 64/144 [01:04<01:21,  1.02s/it] 45%|████▌     | 65/144 [01:06<01:20,  1.02s/it] 46%|████▌     | 66/144 [01:07<01:19,  1.02s/it] 47%|████▋     | 67/144 [01:08<01:18,  1.02s/it] 47%|████▋     | 68/144 [01:09<01:17,  1.02s/it] 48%|████▊     | 69/144 [01:10<01:16,  1.02s/it] 49%|████▊     | 70/144 [01:11<01:15,  1.02s/it] 49%|████▉     | 71/144 [01:12<01:14,  1.02s/it] 50%|█████     | 72/144 [01:13<01:13,  1.02s/it] 51%|█████     | 73/144 [01:14<01:12,  1.02s/it] 51%|█████▏    | 74/144 [01:15<01:11,  1.02s/it] 52%|█████▏    | 75/144 [01:16<01:10,  1.02s/it] 53%|█████▎    | 76/144 [01:17<01:09,  1.02s/it] 53%|█████▎    | 77/144 [01:18<01:08,  1.02s/it] 54%|█████▍    | 78/144 [01:19<01:07,  1.02s/it] 55%|█████▍    | 79/144 [01:20<01:06,  1.02s/it] 56%|█████▌    | 80/144 [01:21<01:05,  1.02s/it]                                                 56%|█████▌    | 80/144 [01:21<01:05,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
{'eval_loss': 1.3482823371887207, 'eval_runtime': 0.458, 'eval_samples_per_second': 21.834, 'eval_steps_per_second': 6.55, 'epoch': 0.83}
{'loss': 1.2504, 'grad_norm': 15.894680976867676, 'learning_rate': 4.444444444444444e-07, 'epoch': 1.67}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 11.19it/s][A                                                
                                             [A 56%|█████▌    | 80/144 [01:21<01:05,  1.02s/it]
100%|██████████| 3/3 [00:00<00:00, 11.19it/s][A2024-11-01 06:08:08,789 - INFO - Evaluation loss: 1.306647539138794

                                             [A 56%|█████▋    | 81/144 [01:22<01:12,  1.16s/it] 57%|█████▋    | 82/144 [01:23<01:09,  1.11s/it] 58%|█████▊    | 83/144 [01:24<01:06,  1.09s/it] 58%|█████▊    | 84/144 [01:25<01:03,  1.07s/it] 59%|█████▉    | 85/144 [01:26<01:01,  1.05s/it] 60%|█████▉    | 86/144 [01:27<01:00,  1.04s/it] 60%|██████    | 87/144 [01:28<00:58,  1.03s/it] 61%|██████    | 88/144 [01:29<00:57,  1.03s/it] 62%|██████▏   | 89/144 [01:30<00:56,  1.02s/it] 62%|██████▎   | 90/144 [01:31<00:55,  1.02s/it] 63%|██████▎   | 91/144 [01:32<00:54,  1.02s/it] 64%|██████▍   | 92/144 [01:33<00:53,  1.02s/it] 65%|██████▍   | 93/144 [01:34<00:52,  1.02s/it] 65%|██████▌   | 94/144 [01:35<00:50,  1.02s/it] 66%|██████▌   | 95/144 [01:36<00:49,  1.02s/it] 67%|██████▋   | 96/144 [01:37<00:43,  1.09it/s] 67%|██████▋   | 97/144 [01:38<00:44,  1.06it/s] 68%|██████▊   | 98/144 [01:39<00:44,  1.03it/s] 69%|██████▉   | 99/144 [01:40<00:44,  1.02it/s] 69%|██████▉   | 100/144 [01:41<00:43,  1.01it/s] 70%|███████   | 101/144 [01:42<00:43,  1.00s/it] 71%|███████   | 102/144 [01:43<00:42,  1.01s/it] 72%|███████▏  | 103/144 [01:44<00:41,  1.01s/it] 72%|███████▏  | 104/144 [01:45<00:40,  1.01s/it] 73%|███████▎  | 105/144 [01:46<00:39,  1.01s/it] 74%|███████▎  | 106/144 [01:47<00:38,  1.01s/it] 74%|███████▍  | 107/144 [01:48<00:37,  1.02s/it] 75%|███████▌  | 108/144 [01:49<00:36,  1.02s/it] 76%|███████▌  | 109/144 [01:50<00:35,  1.02s/it] 76%|███████▋  | 110/144 [01:51<00:34,  1.02s/it] 77%|███████▋  | 111/144 [01:52<00:33,  1.02s/it] 78%|███████▊  | 112/144 [01:53<00:32,  1.02s/it] 78%|███████▊  | 113/144 [01:54<00:31,  1.02s/it] 79%|███████▉  | 114/144 [01:55<00:30,  1.02s/it] 80%|███████▉  | 115/144 [01:56<00:29,  1.02s/it] 81%|████████  | 116/144 [01:58<00:28,  1.02s/it] 81%|████████▏ | 117/144 [01:59<00:27,  1.02s/it] 82%|████████▏ | 118/144 [02:00<00:26,  1.02s/it] 83%|████████▎ | 119/144 [02:01<00:25,  1.02s/it] 83%|████████▎ | 120/144 [02:02<00:24,  1.02s/it]                                                  83%|████████▎ | 120/144 [02:02<00:24,  1.02s/it]
***** Running Evaluation *****
  Num examples = 10
  Batch size = 4
{'eval_loss': 1.306647539138794, 'eval_runtime': 0.4622, 'eval_samples_per_second': 21.637, 'eval_steps_per_second': 6.491, 'epoch': 1.67}
{'loss': 1.1343, 'grad_norm': 18.772462844848633, 'learning_rate': 1.6666666666666665e-07, 'epoch': 2.5}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 11.17it/s][A                                                 
                                             [A 83%|████████▎ | 120/144 [02:02<00:24,  1.02s/it]
100%|██████████| 3/3 [00:00<00:00, 11.17it/s][A2024-11-01 06:08:49,609 - INFO - Evaluation loss: 1.2915863990783691

                                             [A 84%|████████▍ | 121/144 [02:03<00:26,  1.16s/it] 85%|████████▍ | 122/144 [02:04<00:24,  1.12s/it] 85%|████████▌ | 123/144 [02:05<00:22,  1.09s/it] 86%|████████▌ | 124/144 [02:06<00:21,  1.07s/it] 87%|████████▋ | 125/144 [02:07<00:19,  1.05s/it] 88%|████████▊ | 126/144 [02:08<00:18,  1.04s/it] 88%|████████▊ | 127/144 [02:09<00:17,  1.03s/it] 89%|████████▉ | 128/144 [02:10<00:16,  1.03s/it] 90%|████████▉ | 129/144 [02:11<00:15,  1.03s/it] 90%|█████████ | 130/144 [02:12<00:14,  1.03s/it] 91%|█████████ | 131/144 [02:13<00:13,  1.02s/it] 92%|█████████▏| 132/144 [02:14<00:12,  1.02s/it] 92%|█████████▏| 133/144 [02:15<00:11,  1.02s/it] 93%|█████████▎| 134/144 [02:16<00:10,  1.02s/it] 94%|█████████▍| 135/144 [02:17<00:09,  1.02s/it] 94%|█████████▍| 136/144 [02:18<00:08,  1.02s/it] 95%|█████████▌| 137/144 [02:19<00:07,  1.02s/it] 96%|█████████▌| 138/144 [02:20<00:06,  1.02s/it] 97%|█████████▋| 139/144 [02:21<00:05,  1.02s/it] 97%|█████████▋| 140/144 [02:22<00:04,  1.02s/it] 98%|█████████▊| 141/144 [02:23<00:03,  1.02s/it] 99%|█████████▊| 142/144 [02:24<00:02,  1.02s/it] 99%|█████████▉| 143/144 [02:25<00:01,  1.02s/it]100%|██████████| 144/144 [02:26<00:00,  1.10it/s]Saving model checkpoint to logs/checkpoint-144
Configuration saved in logs/checkpoint-144/config.json
Configuration saved in logs/checkpoint-144/generation_config.json
Model weights saved in logs/checkpoint-144/model.safetensors
tokenizer config file saved in logs/checkpoint-144/tokenizer_config.json
Special tokens file saved in logs/checkpoint-144/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 144/144 [02:39<00:00,  1.10it/s]100%|██████████| 144/144 [02:39<00:00,  1.10s/it]
Saving model checkpoint to logs
Configuration saved in logs/config.json
Configuration saved in logs/generation_config.json
Model weights saved in logs/model.safetensors
tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
tokenizer config file saved in logs/tokenizer_config.json
Special tokens file saved in logs/special_tokens_map.json
2024-11-01 06:09:30,854 - INFO - Model and tokenizer saved to logs
{'eval_loss': 1.2915863990783691, 'eval_runtime': 0.4612, 'eval_samples_per_second': 21.683, 'eval_steps_per_second': 6.505, 'epoch': 2.5}
{'train_runtime': 159.0943, 'train_samples_per_second': 3.564, 'train_steps_per_second': 0.905, 'train_loss': 1.3403779930538602, 'epoch': 3.0}
